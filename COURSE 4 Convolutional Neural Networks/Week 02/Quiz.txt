1) nH and nW decrease, while nC increases

2) - Multiple CONV layers followed by a POOL layer
   - FC layers in the last few layers
   
3) False

4) False

5) a[l] and 0, respectively

6) - Using a skip-connection helps the gradient to backpropagate and thus helps you to train deeper networks
   - The skip-connection makes it easy for the network to learn an identity mapping between the input and the output within the ResNet block.

7) 17

8) - You can use a pooling layer to reduce nH, nW, but not nC.
   - You can use a 1x1 convolutional layer to reduce nC but not nH, nW.

9) - A single inception block allows the network to use a combination of 1x1, 3x3, 5x5 convolutions and pooling.
   - Inception blocks usually use 1x1 convolutions to reduce the input data volume’s size before applying 3x3 and 5x5 convolutions.

10) - Parameters trained for one computer vision task are often useful as pretraining for other computer vision tasks.
    - It is a convenient way to get working an implementation of a complex ConvNet architecture.