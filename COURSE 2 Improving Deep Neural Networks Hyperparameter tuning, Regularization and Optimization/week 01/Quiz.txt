1) 98% train . 1% dev . 1% test

2) Come from the same distribution

3) - Make the Neural Network deeper
   - Increase the number of units in each hidden layer

4) - Increase the regularization parameter lambda
   - Get more training data

5) A regularization technique (such as L2 regularization) that results in gradient descent shrinking the weights on every iteration.

6) Weights are pushed toward becoming smaller (closer to 0)

7) You do not apply dropout (do not randomly eliminate units), and do not keep the 1/keep_prob factor in the calculations used in training.

8) - Reducing the regularization effect
   - Causing the neural network to end up with a lower training set error
   
9) - Dropout
   - L2 regularization
   - Data Augmentation

10) It makes the cost function faster to optimize
