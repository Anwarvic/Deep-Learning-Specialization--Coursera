1) We use it to pass variables computed during forward propagation to the corresponding backward propagation step. It contains useful values for backward propagation to compute derivatives.

2) - size of the hidden layers n[l]
   - learning rate a
   - number of iterations
   - number of layers L in the neural network

3) The deeper layers of a neural network are typically computing more complex features of the input than the earlier layers.

4) False

5) for(i in range(1, len(layer_dims))):
  parameter[‘W’ + str(i)] = np.random.randn(layers[i], layers[i-1])) * 0.01
  parameter[‘b’ + str(i)] = np.random.randn(layers[i], 1) * 0.01

6) The number of layers L is 4. The number of hidden layers is 3.

7) True

8) True

9) - W^[1] will have shape (4, 4)
   - b^[1] will have shape (4, 1)
   - W^[2] will have shape (3, 4)
   - b^[2] will have shape (3, 1)
   - b^[3] will have shape (1, 1)
   - W^[3] will have shape (1, 3)

10) W^[l] has shape (n^[l],n^[l-1])

